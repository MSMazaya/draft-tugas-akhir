\section{Perancangan Perangkat Lunak}
\label{sec:perancangan-perangkat-lunak}

Pengujian hasil akurasi akselerator perangkat keras untuk \ac{RL} perlu diuji menggunakan sebuah kasus permasalahan \ac{RL}. Pada penelitian tugas akhir ini, kasus permasalahan yang dipilih adalah \ac{RL} untuk memecahkan permasalahan labirin \parencite{mini2023understanding}. Permasalahan labirin merupakan permasalahan dimana sebuah agent \ac{RL} diletakkan pada sebuah konfigurasi labirin yang diskrit yang telah dijelaskan pada sub sub-bab \ref{sub:sub-rl} dengan ilustrasi gambar \ref{fig:ilustrasi-RL}. Rangkaian pengujian tersebut, agar dapat diujikan kepada akselerator perangkat keras, memerlukan tiga komponen untuk diimplementasikan sebagai berikut.

\vspace{-5mm}
\begin{enumerate}
	\begin{singlespace}
		\item \textit{Generator} konfigurasi labirin dengan input besar labirin (selanjutnya disebut sebagai dimensi labirin) yang dinamik.
		\item Algoritma yang mampu menjadi \textit{benchmark} akurasi dari hasil implementasi \ac{RL}.
		\item Algoritma dari pembelajaran agen \ac{RL} itu sendiri.
	\end{singlespace}
\end{enumerate}

Ketiga hal tersebut, akan dijelaskan secara sebagai pada sub sub-bab \ref{sec:sub-prim}, \ref{sec:sub-dfs}, dan \ref{sec:sub-pembelajaran-rl}.

\subsection{Algoritma Prim untuk \textit{Generator} Labirin}
\label{sec:sub-prim}

Skema labirin yang akan digunakan untuk melakukan pengujian implementasi \ac{RL} pada perangkat lunak dan perangkat keras akan dihasilkan dari algoritma Prim \parencite{devian2013implementasi}. Algoritma Prim dipilih karena dapat menghasilkan skema labirin yang dinamik berdasarkan masukan dimensi labirin, dengan entropi yang relatif konsisten. Implementasi detail dari algoritma Prim terdapat pada algoritma \ref{alg:prim}. Algoritma \ref{alg:prim}, dapat diilustrasikan sesuai dengan gambar \ref{fig:prim-visual}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{chapter-3/prim-visual.jpg}
	\caption{Visualisasi algoritma prim}
	\label{fig:prim-visual}
\end{figure}

Pada gambar \ref{fig:prim-visual}, halangan berkotak oranye merupakan halangan yang masuk kepada \textit{list\_halangan}dan halangan yang berkotak oranye dengan tengah hijau merupakan \textit{halangan\_kini} pada algoritma \ref{alg:prim}. Keacakan pada algoritma Prim akan dikontrol untuk pengujian, sehingga dapat menghasilkan konfigurasi labirin yang sama berkali-kali untuk masukan dimensi labirin yang sama.

\subsection{Algoritma \ac{DFS} untuk \textit{benchmark} hasil \acl{RL}}
\label{sec:sub-dfs}

\acf{DFS} merupakan algoritma yang digunakan untuk memecahkan banyak permasalahan yang berhubungan dengan data struktur \textit{graph}. Algoritma ini pertama kali dipopulerkan oleh J. Hopcroft dan R. Tarjan pada \parencite{hopcroft1973algorithm} dengan ilustrasi diagram alir algoritma pada gambar \ref{fig:hopcroft-dfs} yang disesuaikan dengan konteks permasalahan labirin. Hasil dari penyesuaian yang dilakukan dijelaskan secara detail pada algoritma \ref{alg:dfs-sw}. Algoritma \ref{alg:dfs-sw} dapat divisualisasikan cara kerjanya sesuai dengan gambar \ref{fig:dfs-visualization}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{chapter-3/dfs-tree.jpg}
	\caption{Visualisasi algoritma \ac{DFS}}
	\label{fig:dfs-visualization}
\end{figure}

Pada gambar \ref{fig:dfs-visualization}, lingkaran merupakan analogi pemanggilan fungsi \ac{DFS} pada algoritma \ref{alg:dfs-sw} dengan teks didalamnya merupakan representasi input dari fungsi \ac{DFS} yaitu baris dan kolom. Garis hijau merupakan penanda jalur tercepat yang dihasilkan oleh DFS yang bisa diambil untuk memecahkan permasalahan labirin yang ada pada ilustrasi gambar \ref{fig:dfs-visualization}. Hasil dari algoritma ini, akan digunakan untuk melakukan \textit{benchmark} dari pengujian algoritma \ac{RL} yang akan dibahas secara detail pada sub sub-bab \ref{sec:sub-pembelajaran-rl}.

\subsection{Algoritma Pembelajaran \acl{RL}}
\label{sec:sub-pembelajaran-rl}

Algoritma pembelajaran \ac{RL} yang digunakan pada penelitian ini diadopsi dari pembelajaran \ac{RL} menggunakan \textit{Q-Learning} dari penelitian \parencite{sutisna2023faraneq} yang dideskripsikan pada algoritma \ref{alg:rl-qlearning}. Fungsi \textit{reward} yang menghasilkan dari algoritma \ref{alg:rl-qlearning} pada baris \ref{algline:q-reward} pada penelitian ini didesain sebagai fungsi skalar yang didesain secara iteratif, cara umum untuk mendesain fungsi \textit{reward} berdasarkan \parencite{hayes2022practical} pada agen \ac{RL}. Detail dari implementasi fungsi \textit{reward} pada penelitian ini terangkum pada Algoritma \ref{alg:rl-reward-function}.

Pada algoritma \ref{alg:rl-reward-function} baris \ref{algline:reward-c1}, reward yang diberikan ($C_1$) akan berupa $punishment$, atau $reward$ yang bernilai negatif, yang digunakan untuk menghentikan agen dari melakukan pergerakan menuju halangan. Lalu, $punishment$ lain berada pada baris \ref{algline:reward-c2} ($C_2$) yang berupa $punishment$ untuk menghentikan agen bergerak ke sel kosong yang pernah dilalui sebelumnya. Satu-satunya $reward$ positif yang didapatkan berada pada baris \ref{algline:reward-c3} ($C_3$) yang digunakan untuk memastikan bahwa agen selalu menuju tujuan, menghindari efek $C_4$. Sedangkan $C_4$, pada baris \ref{algline:reward-c4}, merupakan $punishment$ yang digunakan untuk memastikan agen selalu mengambil jalut tercepat.

Algoritma \ref{alg:rl-reward-function} memiliki kekurangan ketidakstabilan untuk menghasilkan strategi optimal ($\pi^*$) dari permasalahan labirin ini. Hal ini, merupakan hal yang sering terjadi pada desain fungsi $reward$ untuk kasus pemilihan aksi diskrit. Salah satu contoh kasus ketidakstabilan ini dapat dilihat dari hasil plot \textit{cumulative reward} pada gambar \ref{fig:cumulative-reward-faraneq} dari \parencite{sutisna2023faraneq} yang juga menggunakan kasus permasalahan labirin sebagai $benchmark$ dari akselerator yang dikembangkan.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{chapter-3/faraneq-cumulative-rewards.jpg}
	\caption{Grafik \textit{cumulative reward} dari penelitian \parencite{sutisna2023faraneq}}
	\label{fig:cumulative-reward-faraneq}
\end{figure}

Grafik \textit{cumulative reward} yang ditunjukkan pada gambar \ref{fig:cumulative-reward-faraneq} memperlihatkan bahwa hasil \textit{cumulative reward} yang didapatkan dari pembelajaran \ac{RL} dari penelitian \parencite{sutisna2023faraneq} itu masih mengalami ketidakstabilan. Pada kasus lingkungan yang dimodelkan dengan fungsi \textit{reward} yang skalar dan diskrit, hal ini menjadi masalah karena turunnya grafik \textit{cumulative reward} adalah sebuah indikasi terjadinya pengambilan strategi/\textit{policy} yang tidak optimal setelah sebuah iterasi pembelajaran. Secara intrinsik, hal tersebut berarti algoritma pembelajaran tidak berhasil secara stabil menghasilkan strategi optimal atau $\pi^*$. Maka, algoritma \ref{alg:rl-qlearning} diekstensi menggunakan metode memoisasi pintar yang diadopsi dari \parencite{mazaya2024reinforcement} yang dijelaskan pada Algoritma \ref{alg:rl-qmemo}. Inti dari ekstensi algoritma \ref{alg:rl-qmemo} berada pada baris \ref{algline:q-memo-equation} yang melakukan perubahan nilai $maxQ(s_{t+1},a))$ dari \textit{Q-Table} menggunakan Persamaan .

\vspace{-12mm}
\begin{flalign}
	\label{eq:q-memo-overwrite}
	Q(s_{t},a_{t_x}) =	Q(s_{t},a_{t_y}) + \delta &  &
\end{flalign}
\vspace{-12mm}

Pada Persamaan \ref{eq:q-memo-overwrite}, \(a_{t_x}\) adalah aksi yang memberikan nilai \textit{memori Q-Table} yang maksimal pada state $s_t$; \(a_{t_y}\) adalah aksi yang memberikan nilai \textit{Q-Table} yang maksimal pada state $s_t$; \(\delta\) adalah konstanta yang digunakan untuk membuat \(Q(s_{t},a_{t_x})\) di \textit{Q-Table} lebih besar dari \(Q(s_{t},a_{t_y})\) di \textit{memori Q-Table} sehingga menjadikan \(Q(s_{t},a_{t_x})\) nilai maksimal pada \textit{Q-Table}.

Implementasi dari ekstensi pada algoritma \ref{alg:rl-qmemo} bertujuan untuk mendapatkan hasil \textit{Q-Table} yang memiliki $maxQ(s_{t+1},a))$, dari Persamaan \ref{eq:q-learning}, yang stabil tanpa menghilangkan nilai pembelajaran dari iterasi sebelumnya. Gambar \ref{fig:q-memo-ilustration}, memberikan ilustrasi bagaimana cara kerja Algoritma \ref{alg:rl-qmemo} secara detail.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{chapter-3/q-memo-ilustration.jpg}
	\caption{Ilustrasi algoritma \ref{alg:rl-qmemo} dari \parencite{mazaya2024reinforcement}}
	\label{fig:q-memo-ilustration}
\end{figure}

Pada gambar \ref{fig:q-memo-ilustration}, diilustrasikan dua tahapan utama dari Persamaan \ref{eq:q-memo-overwrite}: pemilihan $a_{t_y}$ dan pengubahan nilai  $Q(s_{t},a_{t_x})$ dengan nilai $Q(s_{t},a_{t_y}) + \delta$. Terdapat tiga \textit{heatmap} pada gambar \ref{fig:q-memo-ilustration}. Dari kiri ke kanan, \textit{heatmap} pertama merupakan ilustrasi dari sebuah nilai \textit{Q-Table} dari sebuah iterasi yang mengalami penurunan nilai \textit{cumulative reward}. \textit{Heatmap} kedua merupakan ilustrasi dari memori \textit{Q-Table} dari iterasi sebelumnya dengan nilai yang diberi kotak hitam adalah $Q(s_{t},a_{t_y})$. Lalu, \textit{heatmap} ketiga merupakan ilustrasi dari \textit{Q-Table} dari heatmap pertama yang nilai $Q(s_{t},a_{t_y})$-nya sudah diubah menjadi lebih tinggi daripada $Q(s_{t},a_{t_x})$.

Pengujian pembelajaran agen \ac{RL} menggunakan algoritma \ref{alg:rl-qmemo} dilakukan dengan variasi dimensi labirin dan episode pembelajaran yang kemudian akan dibandingkan hasil performanya dengan implementasi pada akselerator perangkat keras.
